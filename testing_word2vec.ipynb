{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "5d913c02-2ee1-430f-945e-2566d242f227"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "66eaabde-073a-4989-b933-fcc742c488cc"
    }
   },
   "outputs": [],
   "source": [
    "#read labeled data\n",
    "labeled=pd.read_csv('/home/ubuntu/git/word2vec_exp/labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "de9c4855-c047-4607-aaf8-df00e9ab0de7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "29203eb1-3505-48c2-990c-5645bcb19e20"
    }
   },
   "outputs": [],
   "source": [
    "#define function to clean the text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default). For word2vec we don't remove words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #5. Lemmatize\n",
    "    wnl = WordNetLemmatizer()\n",
    "    words = list(map(lambda x: wnl.lemmatize(x), words))\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "9fe4352b-fe8a-4e4d-a230-a445009ed3a0"
    }
   },
   "outputs": [],
   "source": [
    "#before we keep cleaning the text we need to download the punkt tokenizer to split sentences. This command only needs to be run\n",
    "#once\n",
    "#clean the text\n",
    "import nltk.data\n",
    "#nltk.download()   \n",
    "#when the dialog box pops up just write punkt to download the appropiate tokenizer\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "9e6fce34-4fac-4deb-adda-a8082788d367"
    }
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "79be735c-0399-40fa-80ff-489a05814395"
    }
   },
   "outputs": [],
   "source": [
    "#load the appropiate tokenizer\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "094574ac-5b1d-422f-bf86-fdd216b74417"
    }
   },
   "outputs": [],
   "source": [
    "#define function to split reviews into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "86635f6f-5b4c-4dce-8153-8dab69013b43"
    }
   },
   "outputs": [],
   "source": [
    "#see if we can come up with a better version of the review to sentences function\n",
    "def review_to_sentences_v2( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    raw_sentences=[review_to_wordlist(x,remove_stopwords=False) for x in raw_sentences if len(raw_sentences)>0]\n",
    "    #raw_sentences=[item for sublist in raw_sentences for item in sublist]\n",
    "    #raw_sentences=[x for x in raw_sentences]\n",
    "    #sentences = []\n",
    "    #for raw_sentence in raw_sentences:\n",
    "    #    # If a sentence is empty, skip it\n",
    "    #    if len(raw_sentence) > 0:\n",
    "    #        # Otherwise, call review_to_wordlist to get a list of words\n",
    "    #        sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "    #          remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return raw_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "nbpresent": {
     "id": "bc50883d-7401-469b-8701-6a702898b58d"
    }
   },
   "outputs": [],
   "source": [
    "#explore how the data is cleaned\n",
    "#a=review_to_wordlist(labeled['review'].iloc[0],remove_stopwords=False)\n",
    "#print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "e74f8db6-9070-43fb-b2a8-f595469edd24"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['with',\n",
       "  'all',\n",
       "  'this',\n",
       "  'stuff',\n",
       "  'going',\n",
       "  'down',\n",
       "  'at',\n",
       "  'the',\n",
       "  'moment',\n",
       "  'with',\n",
       "  'mj',\n",
       "  'i',\n",
       "  've',\n",
       "  'started',\n",
       "  'listening',\n",
       "  'to',\n",
       "  'his',\n",
       "  'music',\n",
       "  'watching',\n",
       "  'the',\n",
       "  'odd',\n",
       "  'documentary',\n",
       "  'here',\n",
       "  'and',\n",
       "  'there',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'wiz',\n",
       "  'and',\n",
       "  'watched',\n",
       "  'moonwalker',\n",
       "  'again'],\n",
       " ['maybe',\n",
       "  'i',\n",
       "  'just',\n",
       "  'want',\n",
       "  'to',\n",
       "  'get',\n",
       "  'a',\n",
       "  'certain',\n",
       "  'insight',\n",
       "  'into',\n",
       "  'this',\n",
       "  'guy',\n",
       "  'who',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'wa',\n",
       "  'really',\n",
       "  'cool',\n",
       "  'in',\n",
       "  'the',\n",
       "  'eighty',\n",
       "  'just',\n",
       "  'to',\n",
       "  'maybe',\n",
       "  'make',\n",
       "  'up',\n",
       "  'my',\n",
       "  'mind',\n",
       "  'whether',\n",
       "  'he',\n",
       "  'is',\n",
       "  'guilty',\n",
       "  'or',\n",
       "  'innocent'],\n",
       " ['moonwalker',\n",
       "  'is',\n",
       "  'part',\n",
       "  'biography',\n",
       "  'part',\n",
       "  'feature',\n",
       "  'film',\n",
       "  'which',\n",
       "  'i',\n",
       "  'remember',\n",
       "  'going',\n",
       "  'to',\n",
       "  'see',\n",
       "  'at',\n",
       "  'the',\n",
       "  'cinema',\n",
       "  'when',\n",
       "  'it',\n",
       "  'wa',\n",
       "  'originally',\n",
       "  'released'],\n",
       " ['some',\n",
       "  'of',\n",
       "  'it',\n",
       "  'ha',\n",
       "  'subtle',\n",
       "  'message',\n",
       "  'about',\n",
       "  'mj',\n",
       "  's',\n",
       "  'feeling',\n",
       "  'towards',\n",
       "  'the',\n",
       "  'press',\n",
       "  'and',\n",
       "  'also',\n",
       "  'the',\n",
       "  'obvious',\n",
       "  'message',\n",
       "  'of',\n",
       "  'drug',\n",
       "  'are',\n",
       "  'bad',\n",
       "  'm',\n",
       "  'kay',\n",
       "  'visually',\n",
       "  'impressive',\n",
       "  'but',\n",
       "  'of',\n",
       "  'course',\n",
       "  'this',\n",
       "  'is',\n",
       "  'all',\n",
       "  'about',\n",
       "  'michael',\n",
       "  'jackson',\n",
       "  'so',\n",
       "  'unless',\n",
       "  'you',\n",
       "  'remotely',\n",
       "  'like',\n",
       "  'mj',\n",
       "  'in',\n",
       "  'anyway',\n",
       "  'then',\n",
       "  'you',\n",
       "  'are',\n",
       "  'going',\n",
       "  'to',\n",
       "  'hate',\n",
       "  'this',\n",
       "  'and',\n",
       "  'find',\n",
       "  'it',\n",
       "  'boring'],\n",
       " ['some',\n",
       "  'may',\n",
       "  'call',\n",
       "  'mj',\n",
       "  'an',\n",
       "  'egotist',\n",
       "  'for',\n",
       "  'consenting',\n",
       "  'to',\n",
       "  'the',\n",
       "  'making',\n",
       "  'of',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'but',\n",
       "  'mj',\n",
       "  'and',\n",
       "  'most',\n",
       "  'of',\n",
       "  'his',\n",
       "  'fan',\n",
       "  'would',\n",
       "  'say',\n",
       "  'that',\n",
       "  'he',\n",
       "  'made',\n",
       "  'it',\n",
       "  'for',\n",
       "  'the',\n",
       "  'fan',\n",
       "  'which',\n",
       "  'if',\n",
       "  'true',\n",
       "  'is',\n",
       "  'really',\n",
       "  'nice',\n",
       "  'of',\n",
       "  'him',\n",
       "  'the',\n",
       "  'actual',\n",
       "  'feature',\n",
       "  'film',\n",
       "  'bit',\n",
       "  'when',\n",
       "  'it',\n",
       "  'finally',\n",
       "  'start',\n",
       "  'is',\n",
       "  'only',\n",
       "  'on',\n",
       "  'for',\n",
       "  'minute',\n",
       "  'or',\n",
       "  'so',\n",
       "  'excluding',\n",
       "  'the',\n",
       "  'smooth',\n",
       "  'criminal',\n",
       "  'sequence',\n",
       "  'and',\n",
       "  'joe',\n",
       "  'pesci',\n",
       "  'is',\n",
       "  'convincing',\n",
       "  'a',\n",
       "  'a',\n",
       "  'psychopathic',\n",
       "  'all',\n",
       "  'powerful',\n",
       "  'drug',\n",
       "  'lord'],\n",
       " ['why', 'he', 'want', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me'],\n",
       " ['because', 'mj', 'overheard', 'his', 'plan'],\n",
       " ['nah',\n",
       "  'joe',\n",
       "  'pesci',\n",
       "  's',\n",
       "  'character',\n",
       "  'ranted',\n",
       "  'that',\n",
       "  'he',\n",
       "  'wanted',\n",
       "  'people',\n",
       "  'to',\n",
       "  'know',\n",
       "  'it',\n",
       "  'is',\n",
       "  'he',\n",
       "  'who',\n",
       "  'is',\n",
       "  'supplying',\n",
       "  'drug',\n",
       "  'etc',\n",
       "  'so',\n",
       "  'i',\n",
       "  'dunno',\n",
       "  'maybe',\n",
       "  'he',\n",
       "  'just',\n",
       "  'hate',\n",
       "  'mj',\n",
       "  's',\n",
       "  'music',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'cool',\n",
       "  'thing',\n",
       "  'in',\n",
       "  'this',\n",
       "  'like',\n",
       "  'mj',\n",
       "  'turning',\n",
       "  'into',\n",
       "  'a',\n",
       "  'car',\n",
       "  'and',\n",
       "  'a',\n",
       "  'robot',\n",
       "  'and',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'speed',\n",
       "  'demon',\n",
       "  'sequence'],\n",
       " ['also',\n",
       "  'the',\n",
       "  'director',\n",
       "  'must',\n",
       "  'have',\n",
       "  'had',\n",
       "  'the',\n",
       "  'patience',\n",
       "  'of',\n",
       "  'a',\n",
       "  'saint',\n",
       "  'when',\n",
       "  'it',\n",
       "  'came',\n",
       "  'to',\n",
       "  'filming',\n",
       "  'the',\n",
       "  'kiddy',\n",
       "  'bad',\n",
       "  'sequence',\n",
       "  'a',\n",
       "  'usually',\n",
       "  'director',\n",
       "  'hate',\n",
       "  'working',\n",
       "  'with',\n",
       "  'one',\n",
       "  'kid',\n",
       "  'let',\n",
       "  'alone',\n",
       "  'a',\n",
       "  'whole',\n",
       "  'bunch',\n",
       "  'of',\n",
       "  'them',\n",
       "  'performing',\n",
       "  'a',\n",
       "  'complex',\n",
       "  'dance',\n",
       "  'scene',\n",
       "  'bottom',\n",
       "  'line',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'for',\n",
       "  'people',\n",
       "  'who',\n",
       "  'like',\n",
       "  'mj',\n",
       "  'on',\n",
       "  'one',\n",
       "  'level',\n",
       "  'or',\n",
       "  'another',\n",
       "  'which',\n",
       "  'i',\n",
       "  'think',\n",
       "  'is',\n",
       "  'most',\n",
       "  'people'],\n",
       " ['if', 'not', 'then', 'stay', 'away'],\n",
       " ['it',\n",
       "  'doe',\n",
       "  'try',\n",
       "  'and',\n",
       "  'give',\n",
       "  'off',\n",
       "  'a',\n",
       "  'wholesome',\n",
       "  'message',\n",
       "  'and',\n",
       "  'ironically',\n",
       "  'mj',\n",
       "  's',\n",
       "  'bestest',\n",
       "  'buddy',\n",
       "  'in',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'a',\n",
       "  'girl'],\n",
       " ['michael',\n",
       "  'jackson',\n",
       "  'is',\n",
       "  'truly',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'talented',\n",
       "  'people',\n",
       "  'ever',\n",
       "  'to',\n",
       "  'grace',\n",
       "  'this',\n",
       "  'planet',\n",
       "  'but',\n",
       "  'is',\n",
       "  'he',\n",
       "  'guilty'],\n",
       " ['well',\n",
       "  'with',\n",
       "  'all',\n",
       "  'the',\n",
       "  'attention',\n",
       "  'i',\n",
       "  've',\n",
       "  'gave',\n",
       "  'this',\n",
       "  'subject',\n",
       "  'hmmm',\n",
       "  'well',\n",
       "  'i',\n",
       "  'don',\n",
       "  't',\n",
       "  'know',\n",
       "  'because',\n",
       "  'people',\n",
       "  'can',\n",
       "  'be',\n",
       "  'different',\n",
       "  'behind',\n",
       "  'closed',\n",
       "  'door',\n",
       "  'i',\n",
       "  'know',\n",
       "  'this',\n",
       "  'for',\n",
       "  'a',\n",
       "  'fact'],\n",
       " ['he',\n",
       "  'is',\n",
       "  'either',\n",
       "  'an',\n",
       "  'extremely',\n",
       "  'nice',\n",
       "  'but',\n",
       "  'stupid',\n",
       "  'guy',\n",
       "  'or',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'sickest',\n",
       "  'liar'],\n",
       " ['i', 'hope', 'he', 'is', 'not', 'the', 'latter']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text our function that uses list comprenhention vs loop.\n",
    "review_to_sentences_v2(labeled['review'].iloc[0],tokenizer=tokenizer, remove_stopwords=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "7d17b91f-e962-4798-a576-d9cad6fe4aef"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "#create the corpus that will be fed into word2vec, this takes a couple of minutes\n",
    "word2vec_text=labeled['review'].apply(lambda x: review_to_sentences_v2(x,tokenizer=tokenizer, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbpresent": {
     "id": "5fb40bea-bdba-4c3a-be8f-ca3ba38822d0"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['with',\n",
       "  'all',\n",
       "  'this',\n",
       "  'stuff',\n",
       "  'going',\n",
       "  'down',\n",
       "  'at',\n",
       "  'the',\n",
       "  'moment',\n",
       "  'with',\n",
       "  'mj',\n",
       "  'i',\n",
       "  've',\n",
       "  'started',\n",
       "  'listening',\n",
       "  'to',\n",
       "  'his',\n",
       "  'music',\n",
       "  'watching',\n",
       "  'the',\n",
       "  'odd',\n",
       "  'documentary',\n",
       "  'here',\n",
       "  'and',\n",
       "  'there',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'wiz',\n",
       "  'and',\n",
       "  'watched',\n",
       "  'moonwalker',\n",
       "  'again'],\n",
       " ['maybe',\n",
       "  'i',\n",
       "  'just',\n",
       "  'want',\n",
       "  'to',\n",
       "  'get',\n",
       "  'a',\n",
       "  'certain',\n",
       "  'insight',\n",
       "  'into',\n",
       "  'this',\n",
       "  'guy',\n",
       "  'who',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'wa',\n",
       "  'really',\n",
       "  'cool',\n",
       "  'in',\n",
       "  'the',\n",
       "  'eighty',\n",
       "  'just',\n",
       "  'to',\n",
       "  'maybe',\n",
       "  'make',\n",
       "  'up',\n",
       "  'my',\n",
       "  'mind',\n",
       "  'whether',\n",
       "  'he',\n",
       "  'is',\n",
       "  'guilty',\n",
       "  'or',\n",
       "  'innocent']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore the results of the model\n",
    "#we have 266551 sentences\n",
    "print(len(word2vec_text[0]))\n",
    "word2vec_text[0]\n",
    "#len(word2vec_text)\n",
    "flat_list = [item for sublist in word2vec_text for item in sublist]\n",
    "#print (len(flat_list))\n",
    "#print (len(word2vec_text))\n",
    "#print (flat_list[0:2])\n",
    "flat_list[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from pathos.multiprocessing import cpu_count\n",
    "\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-09 21:02:56,194 : INFO : collecting all words and their counts\n",
      "2017-12-09 21:02:56,195 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2017-12-09 21:02:56,621 : INFO : PROGRESS: at sentence #10000, processed 225803 words and 120956 word types\n",
      "2017-12-09 21:02:57,060 : INFO : PROGRESS: at sentence #20000, processed 451892 words and 202870 word types\n",
      "2017-12-09 21:02:57,480 : INFO : PROGRESS: at sentence #30000, processed 671315 words and 270894 word types\n",
      "2017-12-09 21:02:57,925 : INFO : PROGRESS: at sentence #40000, processed 897815 words and 335594 word types\n",
      "2017-12-09 21:02:58,371 : INFO : PROGRESS: at sentence #50000, processed 1116963 words and 392880 word types\n",
      "2017-12-09 21:02:58,789 : INFO : PROGRESS: at sentence #60000, processed 1338404 words and 447337 word types\n",
      "2017-12-09 21:02:59,218 : INFO : PROGRESS: at sentence #70000, processed 1561580 words and 498213 word types\n",
      "2017-12-09 21:02:59,637 : INFO : PROGRESS: at sentence #80000, processed 1780887 words and 546810 word types\n",
      "2017-12-09 21:03:00,076 : INFO : PROGRESS: at sentence #90000, processed 2004996 words and 595573 word types\n",
      "2017-12-09 21:03:00,504 : INFO : PROGRESS: at sentence #100000, processed 2226967 words and 641222 word types\n",
      "2017-12-09 21:03:00,939 : INFO : PROGRESS: at sentence #110000, processed 2446581 words and 685194 word types\n",
      "2017-12-09 21:03:01,425 : INFO : PROGRESS: at sentence #120000, processed 2668776 words and 729462 word types\n",
      "2017-12-09 21:03:01,881 : INFO : PROGRESS: at sentence #130000, processed 2894304 words and 771783 word types\n",
      "2017-12-09 21:03:02,302 : INFO : PROGRESS: at sentence #140000, processed 3107006 words and 810033 word types\n",
      "2017-12-09 21:03:02,756 : INFO : PROGRESS: at sentence #150000, processed 3332628 words and 851524 word types\n",
      "2017-12-09 21:03:03,194 : INFO : PROGRESS: at sentence #160000, processed 3555316 words and 890747 word types\n",
      "2017-12-09 21:03:03,626 : INFO : PROGRESS: at sentence #170000, processed 3778656 words and 928781 word types\n",
      "2017-12-09 21:03:04,057 : INFO : PROGRESS: at sentence #180000, processed 3999237 words and 965750 word types\n",
      "2017-12-09 21:03:04,504 : INFO : PROGRESS: at sentence #190000, processed 4224450 words and 1002470 word types\n",
      "2017-12-09 21:03:04,939 : INFO : PROGRESS: at sentence #200000, processed 4448604 words and 1039017 word types\n",
      "2017-12-09 21:03:05,374 : INFO : PROGRESS: at sentence #210000, processed 4669968 words and 1074709 word types\n",
      "2017-12-09 21:03:05,815 : INFO : PROGRESS: at sentence #220000, processed 4894969 words and 1110497 word types\n",
      "2017-12-09 21:03:06,260 : INFO : PROGRESS: at sentence #230000, processed 5117546 words and 1145205 word types\n",
      "2017-12-09 21:03:06,706 : INFO : PROGRESS: at sentence #240000, processed 5345051 words and 1179870 word types\n",
      "2017-12-09 21:03:07,131 : INFO : PROGRESS: at sentence #250000, processed 5559166 words and 1212895 word types\n",
      "2017-12-09 21:03:07,562 : INFO : PROGRESS: at sentence #260000, processed 5779147 words and 1245507 word types\n",
      "2017-12-09 21:03:07,848 : INFO : collected 1266224 word types from a corpus of 5920725 words (unigram + bigrams) and 266551 sentences\n",
      "2017-12-09 21:03:07,849 : INFO : using 1266224 counts as vocab in Phrases<0 vocab, min_count=10, threshold=20, max_vocab_size=40000000>\n",
      "2017-12-09 21:03:07,849 : INFO : source_vocab length 1266224\n",
      "2017-12-09 21:03:21,670 : INFO : Phraser built with 3686 3686 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import phrases\n",
    "\n",
    "def map_phrases(text_stream):\n",
    "\n",
    "    phrases2 = phrases.Phrases(text_stream, min_count = 10, threshold = 20)\n",
    "\n",
    "    bigram = phrases.Phraser(phrases2)\n",
    "\n",
    "    text_stream = map(lambda x: bigram[x], text_stream)\n",
    "    \n",
    "    return text_stream\n",
    "\n",
    "flat_list2 = map_phrases(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbpresent": {
     "id": "500fc543-e174-4169-aa15-f11bffb541d0"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-09 21:03:39,519 : INFO : collecting all words and their counts\n",
      "2017-12-09 21:03:39,520 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-09 21:03:40,290 : INFO : PROGRESS: at sentence #10000, processed 218651 words, keeping 17543 word types\n",
      "2017-12-09 21:03:41,046 : INFO : PROGRESS: at sentence #20000, processed 437992 words, keeping 24618 word types\n",
      "2017-12-09 21:03:41,789 : INFO : PROGRESS: at sentence #30000, processed 650462 words, keeping 29522 word types\n",
      "2017-12-09 21:03:42,546 : INFO : PROGRESS: at sentence #40000, processed 870253 words, keeping 33590 word types\n",
      "2017-12-09 21:03:43,276 : INFO : PROGRESS: at sentence #50000, processed 1082522 words, keeping 36753 word types\n",
      "2017-12-09 21:03:44,030 : INFO : PROGRESS: at sentence #60000, processed 1297358 words, keeping 39496 word types\n",
      "2017-12-09 21:03:44,782 : INFO : PROGRESS: at sentence #70000, processed 1513774 words, keeping 41884 word types\n",
      "2017-12-09 21:03:45,515 : INFO : PROGRESS: at sentence #80000, processed 1726131 words, keeping 44088 word types\n",
      "2017-12-09 21:03:46,264 : INFO : PROGRESS: at sentence #90000, processed 1943005 words, keeping 46304 word types\n",
      "2017-12-09 21:03:47,004 : INFO : PROGRESS: at sentence #100000, processed 2158114 words, keeping 48173 word types\n",
      "2017-12-09 21:03:47,763 : INFO : PROGRESS: at sentence #110000, processed 2370751 words, keeping 49888 word types\n",
      "2017-12-09 21:03:48,513 : INFO : PROGRESS: at sentence #120000, processed 2586246 words, keeping 51758 word types\n",
      "2017-12-09 21:03:49,273 : INFO : PROGRESS: at sentence #130000, processed 2804817 words, keeping 53337 word types\n",
      "2017-12-09 21:03:49,988 : INFO : PROGRESS: at sentence #140000, processed 3010956 words, keeping 54693 word types\n",
      "2017-12-09 21:03:50,752 : INFO : PROGRESS: at sentence #150000, processed 3229635 words, keeping 56249 word types\n",
      "2017-12-09 21:03:51,499 : INFO : PROGRESS: at sentence #160000, processed 3445411 words, keeping 57691 word types\n",
      "2017-12-09 21:03:52,254 : INFO : PROGRESS: at sentence #170000, processed 3661668 words, keeping 59021 word types\n",
      "2017-12-09 21:03:53,001 : INFO : PROGRESS: at sentence #180000, processed 3875344 words, keeping 60312 word types\n",
      "2017-12-09 21:03:53,760 : INFO : PROGRESS: at sentence #190000, processed 4093467 words, keeping 61487 word types\n",
      "2017-12-09 21:03:54,517 : INFO : PROGRESS: at sentence #200000, processed 4310612 words, keeping 62641 word types\n",
      "2017-12-09 21:03:55,269 : INFO : PROGRESS: at sentence #210000, processed 4525094 words, keeping 63834 word types\n",
      "2017-12-09 21:03:56,032 : INFO : PROGRESS: at sentence #220000, processed 4743129 words, keeping 65041 word types\n",
      "2017-12-09 21:03:56,775 : INFO : PROGRESS: at sentence #230000, processed 4959080 words, keeping 66191 word types\n",
      "2017-12-09 21:03:57,535 : INFO : PROGRESS: at sentence #240000, processed 5179473 words, keeping 67317 word types\n",
      "2017-12-09 21:03:58,249 : INFO : PROGRESS: at sentence #250000, processed 5386835 words, keeping 68420 word types\n",
      "2017-12-09 21:03:58,989 : INFO : PROGRESS: at sentence #260000, processed 5599964 words, keeping 69456 word types\n",
      "2017-12-09 21:03:59,464 : INFO : collected 70123 word types from a corpus of 5737097 raw words and 266551 sentences\n",
      "2017-12-09 21:03:59,465 : INFO : Loading a fresh vocabulary\n",
      "2017-12-09 21:03:59,501 : INFO : min_count=40 retains 7825 unique words (11% of original 70123, drops 62298)\n",
      "2017-12-09 21:03:59,501 : INFO : min_count=40 leaves 5362393 word corpus (93% of original 5737097, drops 374704)\n",
      "2017-12-09 21:03:59,521 : INFO : deleting the raw counts dictionary of 70123 items\n",
      "2017-12-09 21:03:59,523 : INFO : sample=0.001 downsamples 53 most-common words\n",
      "2017-12-09 21:03:59,524 : INFO : downsampling leaves estimated 3812842 word corpus (71.1% of prior 5362393)\n",
      "2017-12-09 21:03:59,524 : INFO : estimated required memory for 7825 words and 300 dimensions: 22692500 bytes\n",
      "2017-12-09 21:03:59,543 : INFO : resetting layer weights\n",
      "2017-12-09 21:03:59,649 : INFO : training model with 4 workers on 7825 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-12-09 21:03:59,651 : WARNING : train() called with an empty iterator (if not intended, be sure to provide a corpus that offers restartable iteration = an iterable).\n",
      "2017-12-09 21:03:59,653 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-09 21:03:59,653 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-09 21:03:59,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-09 21:03:59,655 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-09 21:03:59,656 : INFO : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-12-09 21:03:59,657 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-09 21:03:59,657 : WARNING : supplied example count (0) did not equal expected count (1332755)\n",
      "2017-12-09 21:03:59,658 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-12-09 21:03:59,723 : INFO : saving Word2Vec object under /home/ubuntu/git/word2vec_exp/300features_40minwords_10context_v2, separately None\n",
      "2017-12-09 21:03:59,724 : INFO : not storing attribute syn0norm\n",
      "2017-12-09 21:03:59,726 : INFO : not storing attribute cum_table\n",
      "2017-12-09 21:03:59,911 : INFO : saved /home/ubuntu/git/word2vec_exp/300features_40minwords_10context_v2\n"
     ]
    }
   ],
   "source": [
    "#apply the function and define the model\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(flat_list2, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"/home/ubuntu/git/word2vec_exp/300features_40minwords_10context_v2\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "nbpresent": {
     "id": "bcabebf7-2263-457d-9743-edd24592e584"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see results of model\n",
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "nbpresent": {
     "id": "b388724d-c7bb-4c37-86cb-6ddbaa5f374f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('object', 0.21877896785736084),\n",
       " ('miller', 0.1969459354877472),\n",
       " ('cell_phone', 0.19003674387931824),\n",
       " ('rewrite', 0.18728169798851013),\n",
       " ('armstrong', 0.1788104772567749),\n",
       " ('earliest', 0.17426228523254395),\n",
       " ('ever_seen', 0.1708558350801468),\n",
       " ('smart', 0.17040984332561493),\n",
       " ('condition', 0.17008772492408752),\n",
       " ('facility', 0.16877253353595734)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most similar word\n",
    "model.most_similar(\"tom_cruise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "from gensim.models import word2vec\n",
    "new_model = word2vec.Word2Vec.load(\"/home/ubuntu/git/word2vec_exp/300features_40minwords_10context_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mystery_science', 0.2371596097946167),\n",
       " ('scandal', 0.22928236424922943),\n",
       " ('sens', 0.2198421061038971),\n",
       " ('marshall', 0.21387642621994019),\n",
       " ('checking', 0.2093956023454666),\n",
       " ('absence', 0.20591247081756592),\n",
       " ('then', 0.2005385160446167),\n",
       " ('neat', 0.1951739639043808),\n",
       " ('fight', 0.19483427703380585),\n",
       " ('witch', 0.1925419420003891)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.most_similar('zombie')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
