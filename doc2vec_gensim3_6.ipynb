{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import smart_open\n",
    "import requests as req\n",
    "import re\n",
    "import os\n",
    "import tarfile\n",
    "import gensim\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import save_as_line_sentence\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "print(gensim.models.word2vec.CORPUSFILE_VERSION)  # must be >= 0, i.e. optimized compiled version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will explore some of the basic aspects of the very popular doc2vec technique. The tutorial can be divided on the following sections:\n",
    "\n",
    "1. Explanation of what doc2vec is\n",
    "2. discussion of the different implementations of doc2vec in gensim\n",
    "3. use of the imdb databsase\n",
    "4. clusters different versions\n",
    "5. addendum and exploration of other wordvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/jlealtru/data_files/github/nlp_experiments'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current directory to save it to a string with info about our local directory\n",
    "path=os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir(path+'/aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files available for work\n"
     ]
    }
   ],
   "source": [
    "# We will check if the data already exist in our local enviroment. If not we will download it from the Standford\n",
    "# webpage\n",
    "\n",
    "if os.path.isdir(path+'/aclImdb'):\n",
    "    print('Files available for work')\n",
    "else:\n",
    "    print(\"Downloading IMDB archive...\")\n",
    "    url = u'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    r = req.get(url)\n",
    "    with smart_open('standford', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print('file download complete')\n",
    "    # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "    tar = tarfile.open('standford', mode='r')\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    print('extraction complete, files available for work')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need to parse the files we just downloaded into our machine. We define a function that\n",
    "# looks into a folder and parses the contents into a list. We also need to define the sentiment for\n",
    "# the classification.\n",
    "def get_review_content(path_to_folder, sentiment):\n",
    "    contents=[]\n",
    "    for filename in os.listdir(path_to_folder+sentiment):\n",
    "        with open(path_to_folder+sentiment+'/'+filename, 'rb') as openfile:\n",
    "            content=openfile.read()\n",
    "            contents.append(content)\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with the negative freviews\n",
    "contents_negative_train=get_review_content(path+'/aclImdb/train/', 'neg')\n",
    "contents_negative_test=get_review_content(path+'/aclImdb/test/', 'neg')\n",
    "len(contents_negative_test+contents_negative_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we repeat with the positive reviews\n",
    "contents_positive_train=get_review_content(path+'/aclImdb/train/', 'pos')\n",
    "contents_positive_test=get_review_content(path+'/aclImdb/test/', 'pos')\n",
    "len(contents_positive_test+contents_positive_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now parse the text with undefined \n",
    "contents_undefined=get_review_content(path+'/aclImdb/train/', 'unsup')\n",
    "len(contents_undefined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# we can do this whole thing across a number of processors \n",
    "# initialize pool of processes\n",
    "import multiprocessing\n",
    "p = multiprocessing.cpu_count()\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we are going to develop a pipeline with Spacy to clean and process the data. First we need to load the large\n",
    "# english model.\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main characteristic of SpaCy is the use of the Doc class to hold the documents we will analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-fb61c043c9bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents_positive_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    338\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[1;32m    339\u001b[0m                                                 max_length=self.max_length))\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got bytes)"
     ]
    }
   ],
   "source": [
    "nlp(contents_positive_train[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"I am not a parent, neither am I a male. But I was able to identify with every character's heartaches and pains.<br /><br />This is a movie teenagers should watch. Maybe that way they will start appreciating the value of family again. I'm sorry for those that don't understand the value of love, family and friendship.<br /><br />It was very interesting to watch Patrick Duffy in a different role than that of Bobby Ewing. And it is great to see a 19 year old Ben Affleck giving his best in a moving and sincere performance. He showed at an early age, that he is capable of heartfelt drama. He should be offered more serious roles. Note Hollywoodland... his first serious role in years and he went out and won Best Actor at the Venice Festival in 2006.<br /><br />This movie can be appreciated by people of all ages. Maybe shouldn't be watched by children under 10 because they might get scared that the same may happen to their families, but I recommend it to the entire family.<br /><br />I bought this movie on DVD and have watched it with friends many times. Because it portrays the values that are important in life.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_positive_train[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-PRON-',\n",
       " 'be',\n",
       " 'not',\n",
       " 'a',\n",
       " 'parent',\n",
       " ',',\n",
       " 'neither',\n",
       " 'be',\n",
       " '-PRON-',\n",
       " 'a',\n",
       " 'male',\n",
       " '.',\n",
       " 'but',\n",
       " '-PRON-',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'with',\n",
       " 'every',\n",
       " 'character',\n",
       " \"'s\",\n",
       " 'heartache',\n",
       " 'and',\n",
       " 'pains.<br',\n",
       " '/><br',\n",
       " '/>this',\n",
       " 'be',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'teenager',\n",
       " 'should',\n",
       " 'watch',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'that',\n",
       " 'way',\n",
       " '-PRON-',\n",
       " 'will',\n",
       " 'start',\n",
       " 'appreciate',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'family',\n",
       " 'again',\n",
       " '.',\n",
       " '-PRON-',\n",
       " 'be',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'those',\n",
       " 'that',\n",
       " 'do',\n",
       " 'not',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'love',\n",
       " ',',\n",
       " 'family',\n",
       " 'and',\n",
       " 'friendship.<br',\n",
       " '/><br',\n",
       " '/>it',\n",
       " 'be',\n",
       " 'very',\n",
       " 'interesting',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'patrick',\n",
       " 'duffy',\n",
       " 'in',\n",
       " 'a',\n",
       " 'different',\n",
       " 'role',\n",
       " 'than',\n",
       " 'that',\n",
       " 'of',\n",
       " 'bobby',\n",
       " 'ewing',\n",
       " '.',\n",
       " 'and',\n",
       " '-PRON-',\n",
       " 'be',\n",
       " 'great',\n",
       " 'to',\n",
       " 'see',\n",
       " 'a',\n",
       " '19',\n",
       " 'year',\n",
       " 'old',\n",
       " 'ben',\n",
       " 'affleck',\n",
       " 'give',\n",
       " '-PRON-',\n",
       " 'good',\n",
       " 'in',\n",
       " 'a',\n",
       " 'move',\n",
       " 'and',\n",
       " 'sincere',\n",
       " 'performance',\n",
       " '.',\n",
       " '-PRON-',\n",
       " 'show',\n",
       " 'at',\n",
       " 'an',\n",
       " 'early',\n",
       " 'age',\n",
       " ',',\n",
       " 'that',\n",
       " '-PRON-',\n",
       " 'be',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'heartfelt',\n",
       " 'drama',\n",
       " '.',\n",
       " '-PRON-',\n",
       " 'should',\n",
       " 'be',\n",
       " 'offer',\n",
       " 'more',\n",
       " 'serious',\n",
       " 'role',\n",
       " '.',\n",
       " 'note',\n",
       " 'hollywoodland',\n",
       " '...',\n",
       " '-PRON-',\n",
       " 'first',\n",
       " 'serious',\n",
       " 'role',\n",
       " 'in',\n",
       " 'year',\n",
       " 'and',\n",
       " '-PRON-',\n",
       " 'go',\n",
       " 'out',\n",
       " 'and',\n",
       " 'win',\n",
       " 'good',\n",
       " 'actor',\n",
       " 'at',\n",
       " 'the',\n",
       " 'venice',\n",
       " 'festival',\n",
       " 'in',\n",
       " '2006.<br',\n",
       " '/><br',\n",
       " '/>this',\n",
       " 'movie',\n",
       " 'can',\n",
       " 'be',\n",
       " 'appreciate',\n",
       " 'by',\n",
       " 'people',\n",
       " 'of',\n",
       " 'all',\n",
       " 'age',\n",
       " '.',\n",
       " 'maybe',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'watch',\n",
       " 'by',\n",
       " 'child',\n",
       " 'under',\n",
       " '10',\n",
       " 'because',\n",
       " '-PRON-',\n",
       " 'may',\n",
       " 'get',\n",
       " 'scared',\n",
       " 'that',\n",
       " 'the',\n",
       " 'same',\n",
       " 'may',\n",
       " 'happen',\n",
       " 'to',\n",
       " '-PRON-',\n",
       " 'family',\n",
       " ',',\n",
       " 'but',\n",
       " '-PRON-',\n",
       " 'recommend',\n",
       " '-PRON-',\n",
       " 'to',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'family.<br',\n",
       " '/><br',\n",
       " '/>i',\n",
       " 'buy',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'on',\n",
       " 'dvd',\n",
       " 'and',\n",
       " 'have',\n",
       " 'watch',\n",
       " '-PRON-',\n",
       " 'with',\n",
       " 'friend',\n",
       " 'many',\n",
       " 'time',\n",
       " '.',\n",
       " 'because',\n",
       " '-PRON-',\n",
       " 'portray',\n",
       " 'the',\n",
       " 'value',\n",
       " 'that',\n",
       " 'be',\n",
       " 'important',\n",
       " 'in',\n",
       " 'life',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_=[token.lemma_ for token in nlp(contents_positive_train[11].decode(\"utf-8\"))]\n",
    "test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"I am not a parent, neither am I a male. But I was able to identify with every character's heartaches and pains.<br /><br />This is a movie teenagers should watch. Maybe that way they will start appreciating the value of family again. I'm sorry for those that don't understand the value of love, family and friendship.<br /><br />It was very interesting to watch Patrick Duffy in a different role than that of Bobby Ewing. And it is great to see a 19 year old Ben Affleck giving his best in a moving and sincere performance. He showed at an early age, that he is capable of heartfelt drama. He should be offered more serious roles. Note Hollywoodland... his first serious role in years and he went out and won Best Actor at the Venice Festival in 2006.<br /><br />This movie can be appreciated by people of all ages. Maybe shouldn't be watched by children under 10 because they might get scared that the same may happen to their families, but I recommend it to the entire family.<br /><br />I bought this movie on DVD and have watched it with friends many times. Because it portrays the values that are important in life.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_positive_train[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (u\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        u\"Google in 2007, few people outside of the company took him \"\n",
    "        u\"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        u\"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        u\"worth talking to,” said Thrun, now the co-founder and CEO of \"\n",
    "        u\"online higher education startup Udacity, in an interview with \"\n",
    "        u\"Recode earlier this week.\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Thrun PERSON\n",
      "Udacity PERSON\n",
      "Recode PERSON\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    if entity.label_=='PERSON':\n",
    "        print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have 16 processorss we can use in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will clean the reviews to strip text from stopwords, remove non alphanumeric characters, stemming etc. \n",
    "# We also  make use of the multiprocessing library to parallelize the workload.\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "\n",
    "\n",
    "# define the function to clen the text.\n",
    "def clean_text(text):\n",
    "    # filter ords greater than 120\n",
    "    #wl = filter(lambda x: ord(x) < 128, text)\n",
    "\n",
    "    # pass through pre-processing filter\n",
    "    wl = preprocess_string(text, filters = [stem_text, strip_numeric, \n",
    "                                          strip_punctuation, remove_stopwords, \n",
    "                                          strip_short, strip_non_alphanum])\n",
    "    return wl\n",
    "                    \n",
    "    #clean_text = p.map(clean_text, labeled['review'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now check if we are using the fast version of Gensim that optimizes training times\n",
    "assert gensim.models.doc2vec.FAST_VERSION> -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now clean the text using the predefined function\n",
    "with multiprocessing.Pool(processes=8) as pool:\n",
    "    undefined_clean=pool.map(clean_text, contents_undefined)\n",
    "    train_positive_clean=pool.map(clean_text, contents_positive_train)\n",
    "    train_negative_clean=pool.map(clean_text, contents_negative_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the new interface of gensim and how if saves file in special format to speed up \n",
    "calculations of word vectors specially after using more than 8 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#undefined_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the preprocessed corpus into a single file on disk, using memory-efficient streaming\n",
    "# we do this for unlabeled data, train and test.\n",
    "from gensim.utils import save_as_line_sentence\n",
    "#from smart_open import save_as_line_sentence\n",
    "save_as_line_sentence(undefined_clean, path+'/undefined_clean.txt')\n",
    "save_as_line_sentence(train_positive_clean, path+'/train_positive_clean.txt')\n",
    "save_as_line_sentence(train_negative_clean, path+'/tran_negative_clean.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we now create a dataframe with the reviews and the sentiment.\n",
    "imdb_train=pd.DataFrame({'reviews':contents_positive_train+contents_undefined})\n",
    "len(imdb_train)\n",
    "#imdb_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedLineDocument\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "model_sent = Doc2Vec(corpus_file=path+'/undefined_clean.txt', epochs=25, vector_size=200, \n",
    "                     workers=16,dm=1, dm_mean=1, alpha=0.01, seed=27)\n",
    "sent_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.746666193008423"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vec_for_learning(model, tagged_docs): \n",
    "    sents = tagged_docs.values \n",
    "    targets, regressors = zip(*[(doc.tags[0],\n",
    "                                 model.infer_vector(doc.words, steps=20)) for doc in sents])     return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.760078489780426),\n",
       " (27582, 0.6653116345405579),\n",
       " (47462, 0.6626892685890198),\n",
       " (24728, 0.6495351791381836),\n",
       " (28954, 0.6429033875465393),\n",
       " (43891, 0.6380195617675781),\n",
       " (29857, 0.6364273428916931),\n",
       " (11530, 0.6309794783592224),\n",
       " (1871, 0.6225343942642212),\n",
       " (11240, 0.6160120964050293)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_sent.docvecs.most_similar(4)\n",
    "#contents_undefined[4]\n",
    "#contents_undefined[38715]\n",
    "#from scipy.spatial.distance import cosine\n",
    "\n",
    "a=model_sent.infer_vector(doc_words=b,epochs=30, steps=50,alpha=0.0025 )\n",
    "model_sent.docvecs.most_similar([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "distance() missing 1 required positional argument: 'entity2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-8b44dafe2cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: distance() missing 1 required positional argument: 'entity2'"
     ]
    }
   ],
   "source": [
    "gensim.models.keyedvectors.BaseKeyedVectors.distance(a, model_sent.docvecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(path+'/undefined_clean.txt', 'r')\n",
    "firstLine = infile.readline()\n",
    "b=firstLine.split()\n",
    "#==undefined_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now use then new interface of gensim to save the data into disk and then feed that information to the model\n",
    "import itertools\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "# defined a function to stream the clean text \n",
    "def processed_corpus(text_to_save):\n",
    "    for index,article in enumerate(text_to_save):\n",
    "        # concatenate all section titles and texts of each Wikipedia article into a single \"sentence\"\n",
    "        print (index)\n",
    "        doc = '\\n'.join(itertools.chain.from_iterable(zip('undefined'+str(index), article)))\n",
    "        yield (doc)\n",
    "        print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
