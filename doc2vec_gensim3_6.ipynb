{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from smart_open import smart_open\n",
    "import requests as req\n",
    "import re\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/jlealtru/data_files/github/nlp_experiments'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current directory to save it to a string with info about our local directory\n",
    "path=os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir(path+'/aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading IMDB archive...\n",
      "file download complete\n",
      "extraction complete, files available for work\n"
     ]
    }
   ],
   "source": [
    "# We will check if the data already exist in our local enviroment. If not we will download it from the Standford\n",
    "# webpage\n",
    "\n",
    "if os.path.isdir(path+'/aclImdb'):\n",
    "    print('Files available for work')\n",
    "else:\n",
    "    print(\"Downloading IMDB archive...\")\n",
    "    url = u'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    r = req.get(url)\n",
    "    with smart_open('standford', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print('file download complete')\n",
    "    # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "    tar = tarfile.open('standford', mode='r')\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    print('extraction complete, files available for work')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need to parse the files we just downloaded into our machine. We define a function that\n",
    "# looks into a folder and parses the contents into a list. We also need to define the sentiment for\n",
    "# the classification.\n",
    "def get_review_content(path_to_folder, sentiment):\n",
    "    contents=[]\n",
    "    for filename in os.listdir(path_to_folder+sentiment):\n",
    "    #print (filename)\n",
    "    #print (os.getcwd())\n",
    "        with open(path_to_folder+sentiment+'/'+filename, 'rb') as openfile:\n",
    "        #openfile.read()\n",
    "            content=openfile.read()\n",
    "            contents.append(content)\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with the negative freviews\n",
    "contents_negative_train=get_review_content(path+'/aclImdb/train/', 'neg')\n",
    "contents_negative_test=get_review_content(path+'/aclImdb/test/', 'neg')\n",
    "len(contents_negative_test+contents_negative_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we repeat with the positive reviews\n",
    "contents_positive_train=get_review_content(path+'/aclImdb/train/', 'pos')\n",
    "contents_positive_test=get_review_content(path+'/aclImdb/test/', 'pos')\n",
    "len(contents_positive_test+contents_positive_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now parse the text with undefined \n",
    "contents_undefined=get_review_content(path+'/aclImdb/train/', 'unsup')\n",
    "len(contents_undefined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the text\n",
    "#clean the text\n",
    "import multiprocessing\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "\n",
    "# we can do this whole thing across a number of processors \n",
    "# initialize pool of processes\n",
    "p = multiprocessing.cpu_count()\n",
    "\n",
    "def clean_text(text):\n",
    "    # filter ords greater than 120\n",
    "    #wl = filter(lambda x: ord(x) < 128, text)\n",
    "\n",
    "    # pass through pre-processing filter\n",
    "    wl = preprocess_string(text, filters = [stem_text, strip_numeric, \n",
    "                                          strip_punctuation, remove_stopwords, \n",
    "                                          strip_short, strip_non_alphanum])\n",
    "    return wl\n",
    "                    \n",
    "    #clean_text = p.map(clean_text, labeled['review'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define a function to clean the text data that we can apply to all of our functions\n",
    "import gensim\n",
    "assert gensim.models.doc2vec.FAST_VERSION> -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool(processes=8) as pool:\n",
    "    clean=pool.map(clean_text, contents_undefined)\n",
    "    \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62500"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#we now create a dataframe with the reviews and the sentiment.\n",
    "imdb_train=pd.DataFrame({'reviews':contents_positive_train+contents_undefined})\n",
    "len(imdb_train)\n",
    "#imdb_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
